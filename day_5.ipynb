{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../lxmls-toolkit')\n",
    "import lxmls\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1\n",
    "\n",
    "Go to **```lxmls/deep\\_learning/mlp.py:class NumpyMLP```**  and check the function **```grads()```** and complete the code of the NumpyMLP class with the Backpropagation recursion that we just saw.\n",
    "Once you are done. Try different network geometries by increasing the number of\n",
    "layers and layer sizes e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "import numpy as np\n",
    "import lxmls.readers.sentiment_reader as srs\n",
    "scr = srs.SentimentCorpus(\"books\")\n",
    "\n",
    "train_x = scr.train_X.T\n",
    "train_y = scr.train_y[:, 0]\n",
    "test_x = scr.test_X.T\n",
    "test_y = scr.test_y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load mlp and sgd from toolit\n",
    "from lxmls import deep_learning \n",
    "from lxmls.deep_learning import mlp\n",
    "import lxmls.deep_learning.sgd as sgd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13989, 1600), (1600,))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "geometry = [train_x.shape[0], 100,100, 2]\n",
    "actvfunc = ['sigmoid', 'sigmoid','softmax'] \n",
    "# Instantiate model\n",
    "mlp      = mlp.NumpyMLP(geometry, actvfunc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "n_iter = 5\n",
    "bsize  = 5\n",
    "lrate  = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 321/321 (100%)   Epoch  1/ 5 in 16.31 seg\n",
      "Batch 321/321 (100%)   Epoch  2/ 5 in 16.20 seg\n",
      "Batch 321/321 (100%)   Epoch  3/ 5 in 16.47 seg\n",
      "Batch 321/321 (100%)   Epoch  4/ 5 in 16.54 seg\n",
      "Batch 321/321 (100%)   Epoch  5/ 5 in 16.63 seg\n",
      "\n",
      "MLP ([13989, 100, 100, 2]) Amazon Sentiment Accuracy train: 0.945000 test: 0.797500\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "sgd.SGD_train(mlp, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))\n",
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test  = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "print \"MLP (%s) Amazon Sentiment Accuracy train: %f test: %f\" % (geometry, acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load mlp and sgd from toolit\n",
    "from lxmls import deep_learning \n",
    "from lxmls.deep_learning import mlp\n",
    "import lxmls.deep_learning.sgd as sgd\n",
    "\n",
    "# Model parameters\n",
    "geometry = [train_x.shape[0], 300, 2]\n",
    "actvfunc = ['sigmoid', 'softmax'] \n",
    "# Instantiate model\n",
    "mlp      = mlp.NumpyMLP(geometry, actvfunc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 321/321 (100%)   Epoch  1/ 5 in 48.97 seg\n",
      "Batch 321/321 (100%)   Epoch  2/ 5 in 48.05 seg\n",
      "Batch 321/321 (100%)   Epoch  3/ 5 in 50.11 seg\n",
      "Batch 321/321 (100%)   Epoch  4/ 5 in 49.49 seg\n",
      "Batch 321/321 (100%)   Epoch  5/ 5 in 49.34 seg\n",
      "\n",
      "MLP ([13989, 300, 2]) Amazon Sentiment Accuracy train: 0.960000 test: 0.790000\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "sgd.SGD_train(mlp, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))\n",
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test  = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "print \"MLP (%s) Amazon Sentiment Accuracy train: %f test: %f\" % (geometry, acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2 \n",
    "#### Begin Exercise 5.2\n",
    "\n",
    "Get in contact with Theano. Learn the difference between a symbolic\n",
    "representation and a function. Start by implementing the first layer of our\n",
    "previous MLP in Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy code\n",
    "x        = test_x             # Test set \n",
    "W1, b1   = mlp.params[:2]     # Weights and bias of fist layer \n",
    "z1       = np.dot(W1, x) + b1 # Linear transformation\n",
    "tilde_z1 = 1/(1+np.exp(-z1))  # Non-linear transformation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano code. \n",
    "# NOTE: We use undescore to denote symbolic equivalents to Numpy variables. \n",
    "# This is no Python convention!.\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "_x = T.matrix('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this variable does not have any particular value, nor a space\n",
    "reserved in memory for it. It contains just a symbolic definition of what the\n",
    "variable can contain. The particular values will be given when we use it to\n",
    "compile a function. \n",
    "\n",
    "We could actually use the same definition format to define the weights and give\n",
    "their particular values as inputs to the compiled function. However, since we\n",
    "will be using a more complicated format in later exercises, we will use it here\n",
    "as well. The **```shared```** class allows to define variables that are shared\n",
    "across functions. They are also given a concrete value so that we do not need\n",
    "to give it for each function call. This format is therefore ideal for the\n",
    "weights of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_W1 = theano.shared(value=W1, name='W1', borrow=True) \n",
    "_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets describe the operations we want to do with the variables. Again only\n",
    "symbolically. This is done by replacing our usual operations by Theano symbolic\n",
    "ones when necessary e. g. the internal product dot() or the sigmoid. Some\n",
    "operations like e.g. $+$ are automatically recognized by Theano (operator\n",
    "overloading). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_z1            = T.dot(_W1, _x) + _b1\n",
    "_tilde_z1      = T.nnet.sigmoid(_z1)\n",
    "# Keep in mind that naming variables is useful when debugging\n",
    "_z1.name       = 'z1'\n",
    "_tilde_z1.name = 'tilde_z1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging the code it is often useful to print the graph of computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid [@A] 'tilde_z1'   \n",
      " |Elemwise{add,no_inplace} [@B] 'z1'   \n",
      "   |dot [@C] ''   \n",
      "   | |W1 [@D]\n",
      "   | |x [@E]\n",
      "   |b1 [@F]\n"
     ]
    }
   ],
   "source": [
    "# Perceptron computation graph\n",
    "theano.printing.debugprint(_tilde_z1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to keep in mind that, until this point, we do not have a\n",
    "function we can use to produce any practical input. In order to obtain this we\n",
    "have to compile this function by calling    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1 = theano.function([_x], _tilde_z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of [ ] for the input variables, even if we just specify one\n",
    "variable. We can now do a test to compare the Numpy and Theano implementations\n",
    "and see that they give the same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numpy and Theano Perceptrons are equivalent\n"
     ]
    }
   ],
   "source": [
    "# Check Numpy and Theano match\n",
    "if np.allclose(tilde_z1, layer1(x.astype(theano.config.floatX))):\n",
    "    print \"\\nNumpy and Theano Perceptrons are equivalent\"\n",
    "else:\n",
    "    raise ValueError, \"Numpy and Theano Perceptrons are different\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End exercise 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbolic forward pass\n",
    "In the previous section you have seen how to create symbolic Theano functions\n",
    "with shared parameters. You have thus all you need to implement the whole\n",
    "forward pass of a generic MLP in Theano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.3\n",
    "\n",
    "#### Begin Exercise 5.3\n",
    "\n",
    "Complete the method **```_forward()```** inside of the **```lxmls/deep\\_learning/mlp.py```**, in the class TheanoMLP.\n",
    "\n",
    "Note that this is called only once at the initialization of the\n",
    "class. To debug your implementation put a breakpoint at the \\_\\_init\\_\\_\n",
    "function call. Hint: Note that this is very similar to NumpyMLP.forward().\n",
    "You just need to keep track of the symbolic variable representing the output of\n",
    "the network after each layer is applied and compile the function at the end.\n",
    "After you are finished instantiate a Theano class and check that Numpy and\n",
    "Theano forward pass are the same. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxmls.deep_learning import mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_a = mlp.NumpyMLP(geometry, actvfunc)\n",
    "mlp_b = mlp.TheanoMLP(geometry, actvfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End Exercise 5.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic differentiation\n",
    "In the previous section we compiled the forward pass of a MLP. In this section\n",
    "we will do the same with the cost used for training. We will also derive the\n",
    "gradients although this will be trivial once we have the cost function compiled.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.4\n",
    "\n",
    "We first see an example that does not use any of the code in TheanoMLP but\n",
    "rather continues from what you wrote in exercise 6.3. In this exercise you\n",
    "completed a sigmoid layer with Theano. To get some values for the weights we\n",
    "used the first layer of the network you trained in 6.2. now we are going to use\n",
    "the second layer as well. This is thus assuming that your network in 6.2 has\n",
    "only two layers e.g. the recommended geometry (I, 20, 2). Make sure this is the\n",
    "case before starting this exercise.  \n",
    "\n",
    "For the sake of clarity, lets write here the part of Ex. 5.2 that we had completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the values from our MLP from Ex 6.2\n",
    "W1, b1   = mlp_a.params[:2]     # Weights and bias of fist layer \n",
    "# First layer symbolic variables\n",
    "_x  = T.matrix('x')\n",
    "_W1 = theano.shared(value=W1, name='W1', borrow=True) \n",
    "_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True)) \n",
    "# First layer symbolic expressions\n",
    "_z1       = T.dot(_W1, _x) + _b1\n",
    "_tilde_z1 = T.nnet.sigmoid(_z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "Y = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_1_shape = (2,3)\n",
    "W_2_shape = (3,2)\n",
    "\n",
    "W_1 = np.random.normal(loc=0,scale=0.1, size=W_1_shape)\n",
    "b_1 = np.zeros(3)\n",
    "\n",
    "W_2 = np.random.normal(loc=0,scale=0.1, size=W_2_shape)\n",
    "b_2 = np.zeros(2)\n",
    "\n",
    "weights = [W_1, W_2]\n",
    "biases = [b_1,b_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class activation_layer(object):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    z = np.array(z)\n",
    "    return z*(z>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu([5,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax (z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98201379,  0.01798621])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([5,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activations = [relu, softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CrossEntropy(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"\\nTraining cost: CrossEntropy \")\n",
    "\n",
    "    def compute_cost(self, pred, target):\n",
    "        \"\"\"\n",
    "        Return the cost associated for \"Xpred\" and desired output \"Xtarget\".\n",
    "        Note that the function is vectorized and\n",
    "           - rows in pred are training examples. \n",
    "           - rows in target are output predictions\n",
    "\n",
    "        \"\"\"\n",
    "        return np.mean((target - pred)**2)\n",
    "\n",
    "    def delta(self, preactivation, activation, target):\n",
    "        \"\"\"\n",
    "        Return the error delta from the output layer.\n",
    "        \"\"\"\n",
    "        # nablaC_activation = (activation-target)\n",
    "        delta = 0.5 * (activation - target) #* sigmoid_prime(preactivation)\n",
    "        return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training cost: MSE \n"
     ]
    }
   ],
   "source": [
    "mse = MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_propagate(x, weights, biases, activations):\n",
    "    \"\"\"\n",
    "    Computes the activations of the first hidden layer and output layer.\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    \n",
    "    z_1 = np.dot(x,W1) + b_1\n",
    "    z_1_tilde = act1(z1)\n",
    "    activations.append(z_1_tilde )\n",
    "    \n",
    "    z_2 = np.dot(z_1_tilde, W2) + b_2\n",
    "    z_2_tilde =  act2(z_2)\n",
    "    activations.append(z_2_tilde)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_grad(activations, target, cost):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    output = activations[-1]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
