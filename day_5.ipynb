{
 "metadata": {
  "name": "",
  "signature": "sha256:e871c5437ee09466b69dc932e927b23dcb1e4c5c68dcda6344a114066c821ddc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Day 5: Big Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('/Users/davidbuchacaprats/Dropbox/lxmls2015/lxmls-toolkit-student')\n",
      "import lxmls\n",
      "import scipy\n",
      "path_inside_lxmls_toolkit_student = \"/Users/davidbuchacaprats/Dropbox/lxmls2015/lxmls-toolkit-student\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd lxmls-toolkit_fork/lxmls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "README.md    day 3.ipynb  day_2.ipynb  day_4.ipynb  day_5.ipynb\r\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In today\u2019s main assignment we will show you how to solve a simple problem (counting words in documents) in a distributed manner. We will then ask you to implement a distributed solution for a problem you already know (Na\u00efve Bayes classification).\n",
      "\n",
      "As extra work, we also ask that you implement the EM algorithm (which was extra work in Day 2) in a distributed manner. This is a challenging exercise, even without the time limit of these lab sessions!\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Introduction to map reduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The initial idea of MapReduce was an implementation by Google, but very soon Hadoop appeared as an open-source implementation of the same paradigm.\n",
      "\n",
      "The basic idea is to take your original problem, whichever it may be, and tackle it in two steps:\n",
      "\n",
      "- **Map step**: Divide the data into several parts and send each part to a different computer. Each computer does some computation using only that part of the data, and returns some output. It comes from the functional programming world, and is actually present in Python as a builtin function, map.\n",
      "\n",
      "\n",
      "- **Reduce step**: Collect all the outputs from all the different computers and compute the final solution from those outputs. Again, this name comes from the functional programming and it is present in Python as the builtin reduce. However, as we will see the reduce step in MapReduce is a bit more intelligent than the traditional reduce\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**We will explain how MapReduce works using the classic example of application of MapReduce, which is the word count problem:**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- You have a collection of documents, and there are many of them.\n",
      "- You want to count how many times each word appears in the whole collection."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**If the text corpus is small**, this is trivial to do on a single machine. However, for big corpora, one computer alone would take a long time. For example, the whole English Wikipedia is about 10 GB compressed, and several times that when uncompressed. It would take a considerable amount of time to count the occurrence of each word on the whole dataset using only one computer.\n",
      "However, this problem is quite easy to parallelize using the MapReduce framework:\n",
      "\n",
      "1. **You process each document by itself, counting how many times each word appears**. This is the map step. Notice that each document can be processed in parallel, by multiple machines. The result for each file is a dictionary of a count for each word.\n",
      "\n",
      "2. **You sum up the counts for each word to get the final count**. This is the reduce step. Notice that you can now parallelize over words.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Keys, Values, and the MRJob package"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is stated above is a bit of an over-simplification. \n",
      "\n",
      "In MapReduce, **the map step actually outputs zero or more pairs** of the form (key, value).\n",
      "\n",
      "Let\u2019s suppose that you want to run the wordcount example in the following two documents, each of which contains only one sentence:\n",
      "\n",
      "```\n",
      "the center of Lisbon is the best part of the city\n",
      "the campus of IST is located near Alameda\n",
      "```\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose that you send each sentence to a separate worker. The output of the first worker (which is processing the first sentence) would be:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Key | Value\n",
      "------------ | -------------\n",
      "\"the\"        | 3\n",
      "\"center\"     | 1\n",
      "\"of\"         | 2\n",
      "...          | ...\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and the output of the second worker (which is processing the second sentence) would be:\n",
      "\n",
      "Key | Value\n",
      "------------ | -------------\n",
      "\"the\"        | 1\n",
      "\"campus\"     | 1\n",
      "\"of\"         | 1\n",
      "...          | ...\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Keys can be anything you want: when counting words, it makes sense to have each worker use words as keys, and values as the counts of those words in the text that was passed onto that worker.\n",
      "\n",
      "**In the reduce step, the reducer will receive these key/value pairs and do something with them.** In word counting, the natural operation for the reduce step is to sum the counts with the same key, in order to produce this:\n",
      "\n",
      "Key | Value\n",
      "------------ | -------------\n",
      "\"the\"        | 4\n",
      "\"center\"     | 1\n",
      "\"campus\"     | 1\n",
      "\"of\"         | 3\n",
      "...          | ...\n",
      "\n",
      "Note that the reduce step also produces key/value pairs. As you can see, this last table is the correct output of the Word Count algorithm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this tutorial, we will be using the package MrJob"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "MRJob"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " In MRJob, you should define mapper and reducer functions which implement the map and reduce steps. Whenever you are ready to produce a key/value pair, you should use the Python instruction yield key, value, where key and value are the variables containing the key and value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mrjob.job import MRJob\n",
      "#help(MRJob)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "About yield in python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Without going into too much detail, the yield instruction in Python is used for generators, which are list-like objects which are more memory efficient. \n",
      "\n",
      "Instead of storing all the values in memory like a list, a generator simply knows how to produce the next element in a sequence. \n",
      "\n",
      "For example:\n",
      "- ```range(5)``` produces a list with the values 0 to 4. \n",
      "- ```xrange(5)``` produces a generator which knows that its first value is 0 and that following values get incremented by 1 each time, up to and including 4, but these values are never stored together in memory. \n",
      "\n",
      "If you want to learn more about generators in Python, visit http://docs.python.org/2/tutorial/classes.html \u2013 but you probably don\u2019t need to read that for this lesson."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "this_is_a_list = range(5)\n",
      "this_is_a_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 85,
       "text": [
        "[0, 1, 2, 3, 4]"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "this_is_not_a_list = xrange(5)\n",
      "this_is_not_a_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 88,
       "text": [
        "xrange(5)"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(this_is_not_a_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 89,
       "text": [
        "5"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Running Word Count on one machine"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is an implementation of the Word Count algorithm in Python, using MRJob:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import the necessary libraries:\n",
      "from mrjob.job import MRJob\n",
      "\n",
      "class WordCount(MRJob):\n",
      "    def mapper(self, _, doc):\n",
      "        c = {}\n",
      "        \n",
      "        # Process the document for w in doc.split():\n",
      "        if w in c: \n",
      "            c[w] += 1\n",
      "        else:\n",
      "            c[w] = 1\n",
      "       \n",
      "        # Now, output the results\n",
      "        for w,c in c.items(): \n",
      "            yield w,c\n",
      "\n",
      "    def reducer(self, key, cs): \n",
      "        yield key, sum(cs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#wc = WordCount() # This can't be instanciated WHY???\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This file already exists as ```wordcount.py``` (inside the ```big_data``` folder).\n",
      "\n",
      "Navigate to the big data folder and type this into a terminal:\n",
      "\n",
      "``python wordcount.py ../../data/wikipedia/en_perline001.txt > results.txt``\n",
      "\n",
      "**This command will use the WordCount class already defined with the dataset en_perline001.txt and will write the output of the counts in reusults.txt**\n",
      "\n",
      "Open the results.txt file in your favorite text editor. Each line of this file contains a word and the corresponding count.\n",
      "\n",
      "The last 4 lines that appear in results.txt are:\n",
      "\n",
      "```\n",
      "\"zweites\"     1\n",
      "\"zwischen\"    5\n",
      "\"zymogram\"    1\n",
      "\"zymography\"  4```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are a few important points which you need to remember for the exercise of this session:\n",
      "\n",
      "- By default, MRJob splits the input (the ```en_perline001.txt``` file) by lines. This means that each line may be sent to a different worker. We have taken care of this detail for you, but when you use MRJob for your own projects, remember that the input is split by lines unless you configure it differently. In the future, when we say that we \u201dsplit the input into documents\u201d, we will be referring to this split by lines.\n",
      "\n",
      "\n",
      "- You must create a class and make it inherit from the MRJob class, as we did in the line\n",
      "\ufffc      ```class WordCount(MRJob):```\n",
      "\n",
      "\n",
      "- You should create functions with the special names ```mapper``` and ```reducer```. These functions always have three inputs: ```self```, which all class functions have, ```key```, and ```value```. \n",
      "** WE SHOULD SAY THAT THE REDUCER WILL ALWAYS HAVE THOSE INPUTS NOT THOSE FUNCTIONS **.In our case, the mapper\u2019s input is just text which doesn\u2019t have a key; in Python, it is common to use an underscore, , to denote unused input/output arguments.\n",
      "\n",
      "\n",
      "- The ```mapper``` function takes the input text (the ```doc``` argument), splits it into words using the command ```doc.split()```, and then counts the words in that text.\n",
      "\n",
      "\n",
      "-  The ```reducer``` function has a ```key``` argument which is just a string containing a word. It also has a ```cs``` argument, which is a generator object. For most purposes, you can treat this argument as a list of all the counts in all the documents of the word in the ```key``` argument. Summing all the values in ```cs``` yields the output of the Word Count algorithm for this word.\n",
      "\n",
      "We suggest that you spend some time studying and understanding this deceptively simple code.\n",
      "\n",
      "Try putting the Python line import pdb; pdb.set trace() at multiple places within the code to see what is contained in each variable.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Running Word Count on Amazon EC2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using Na\u00efve Bayes for Language Detection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Language detection is surprisingly easy if you have enough data to train your system. In our case, we\u2019re going to use triplets of characters (or \u201dtrimers\u201d) as features. For example, if your whole training corpus is a sentence in English which reads \u201dI love Lisbon\u201d (length: 13 characters) and a sentence in Portuguese which reads \u201dAdoro Lisboa\u201d (length: 12 characters), you would say that you saw the following features: \u201dI l\u201d, \u201d lo\u201d, \u201dlov\u201d, \u201dove\u201d, \u201dve \u201d, and so on in the English data, and \u201dAdo\u201d, \u201ddor\u201d, \u201doro\u201d, \u201dro \u201d, and so on in the Portuguese data. Note the presence of whitespace on some of these trimers.\n",
      "\n",
      "You can now use the exact same Na \u0308\u0131ve Bayes algorithm which you used for sentiment analysis on Day 1. Instead of classifying text into two classes (positive and negative) we\u2019re going to classify text into two other classes (Portuguese and English). Our training data will be parts of the Portuguese and English Wikipedias.\n",
      "\n",
      "The implementation of distributed Na \u0308\u0131ve Bayes is very similar to counting words:\n",
      "\n",
      "- The map step takes the whole training data of the Portuguese language and splits it into documents. The mapper function computes the frequency of each trimer on one document.\n",
      "\n",
      "\n",
      "- The reduce step compiles the information from all the documents and sums the counts of every Por- tuguese document.\n",
      "\n",
      "\n",
      "- The previous two steps are repeated for the English training data.\n",
      "\n",
      "\n",
      "- After the MapReduce part, a post-processing step uses these counts with similar formulas as in Day 1 to classify new unseen text into the two classes: English or Portuguese. For convenience, we repeat \n",
      "these formulas below.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once you have the counts of trimer occurrences on each language, you need to estimate:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- The ```prior``` probability of each language appearing at test time, ${\\hat P}(\\text{PT})$ and ${\\hat P}(\\text{EN})$. For simplicity, instead of using Maximum Likelihood estimation, let's assume that the users of your language detector are equally likely to try English and Portuguese sentences. Thus, ${\\hat P}(\\text{PT}) = {\\hat P}(\\text{EN}) = \\frac{1}{2}$.\n",
      "\n",
      "\n",
      "- The probability of each feature (trimer) given the class, ${\\hat P}(t_j | c)$. We will again use Maximum Likelihood estimation, which means that the probability of trimer $t$ given language $c$ is equal to the number of times $t$ occurred in documents of language $c$, divided by the total number of trimers in language $c$. Mathematically: $${\\hat P}(t_j|\\text{PT}) = \\frac{n(t_j,\\text{PT})}{\\sum_j n(t_j,\\text{PT})},$$ where $n(t_j,\\text{PT})$ is the number of times the trimer $t_j$ occurred in your Portuguese training corpus. A similar formula is used for ${\\hat P}(t_j|\\text{EN})$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then, given a test sentence $x$, we can compute the following argmax for the two classes $c = \\text{PT}$ and $c = \\text{EN}$:\n",
      "\n",
      "$(*)$\n",
      "$$  \n",
      "\\text{argmax}_c {\\hat P}(c | x) = \\text{argmax}_c {\\hat P}(c) {\\hat P}(x | c) = \\text{argmax}_c {\\hat P}(c) \\prod_j {\\hat P}(t_j | c) \n",
      "$$\n",
      "\n",
      "$$\n",
      "= \\text{argmax}_c \\prod_j {\\hat P}(t_j | c) = \\text{argmax}_c \\sum_j \\log({\\hat P}(t_j | c))\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the first equality we used Bayes' Theorem.\n",
      "\n",
      "In the second one we used the assumption of conditional independence of features given the class, which is at the core of Na\u00efve Bayes.\n",
      "\n",
      "In the third one, we used that ${\\hat P}(\\text{PT}) = {\\hat P}(\\text{EN}) = \\frac{1}{2}$, thus the prior does not affect the argmax. \n",
      "\n",
      "In the last equation, we used the fact that the argmax is not affected by the application of a logarithm. Logarithms will prevent your program from encountering underflow errors when multiplying many numbers which are very small.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Main Assignment: Distributed Na\u00efve Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Using the Word Count example we\u2019ve given you as reference, implement the Na \u0308\u0131ve Bayes language detector described above. You should do this in two parts:\n",
      "\n",
      "- Steps 1 to 3 (counting occurrences of trimers in train data, for both languages), should be run locally with MRJob. You should save the result into a file, just like we did in the Word Count example using the redirect operator >.\n",
      "\n",
      "- Step 4 should be run using a separate Python script which does not use MRJob (it is quite fast even with just one computer, even if you had used a lot more data). It should implement the formula given in equation (*)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You must run steps 1-3 twice, once on a selection of the Portuguese Wikipedia (1%) and another on the English Wikipedia (only 0.1% as this is much larger). \n",
      "\n",
      "These files already exist on the LXMLS Toolkit as pt ```perline01.txt``` and en perline001.txt, in the ```data/wikipedia``` folder. They are small enough to run on your local machine only, and should already produce a decent language detector.\n",
      "\n",
      "Steps 1-3 are very similar to the word count code which we gave you.\n",
      "The main difference is that you are splitting the document into trimers instead of words.\n",
      "\n",
      "The main difference is that you are splitting the document into trimers instea dofwords.Remembertousethesyntax> en.counts.txtand> pt.counts.txt when you run the two jobs, to output the results to appropriate files.\n",
      "If your code is in file trimercount.py, inside the big data folder, the two commands you need to type into the terminal to run things in your AWS machine are:\n",
      "\n",
      "    python trimercount.py ../../data/wikipedia/en_perline001.txt > en.counts.txt\n",
      "    \n",
      "    python trimercount.py ../../data/wikipedia/pt_perline01.txt > pt.counts.txt\n",
      "    \n",
      "Step 4 should use the two files en.counts.txt and pt.counts.txt to implement the language detector. For this part you don\u2019t need to use MRJob and you can use plain Python as you did in the previous lab sessions. If you don\u2019t have time to implement this part, you can use our own implementation in file postprocess.py.\n",
      "\n",
      "To check if things worked, try the command:\n",
      "\n",
      "    grep acc en.counts.txt\n",
      "    \n",
      "which should tell you that the trimer ```acc``` appeared 3448 times in the English file.\n",
      "To test your language detector, see if it classifies correctly some sentences which you make up. If you do not know Portuguese, here are a few sentences you can use to test:\n",
      "\n",
      "- Esperamos que estejam a gostar desta Escola.\n",
      "\n",
      "\n",
      "- Se tiverem qualquer comenta \u0301rio a fazer, por favor enviem-nos um email.\n",
      "\n",
      "\n",
      "- Os organizadores gostariam de agradecer aos monitores das aulas pr\u00e1ticas pela grande ajuda na elabora\u00e7\u00e3o desta aula.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Extra Work: Distributed EM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "EM is an iterative method: for T iterations, we have to alternate between the E-Step and the M-Step. Both steps can be done in a distributed manner; in this lesson, we\u2019re going to focus on a simple way to distribute the E-Step; the M-Step will be non-distributed (at first). To see how to distribute both steps in various config- urations see, e.g., Wolfe et al. (2008).\n",
      "\n",
      "\n",
      "How can we distribute the E-Step? \n",
      "\n",
      "Recall from equations (2.51)\u2013(2.54) that the E-Step involves summing over m, where m indexes each datum in your dataset. In POS induction, m indexes every sentence. The important factor to understand is that each sequence can be processed completely independently of each other (these are the inner expressions) given the previous iterations\u2019 qt. This will correspond to a map step. The sums over m will be the reduce step.\n",
      "\n",
      "\n",
      "In the last lesson, you already saw the Word Count and Na \u0308\u0131ve Bayes algorithms. In these two examples, we counted something in the Map step and then summed them in the Reduce step. That is the intuition behind what we will do today: we will distribute our data to several workers in the map step, count things separately, then sum those counts in the reduce step. Everything else will be exactly the same as in section 2.7.\n",
      "\n",
      "The most important concepts that you should get out of today\u2019s tutorial are:\n",
      "\n",
      "1. Since we can process each sequence independently, this is naturally a map step.\n",
      "2. A full Map/Reduce call will correspond to a single iteration of the algorithm.\n",
      "3. To run multiple iterations, you will need to run multiple MapReduce calls.\n",
      "4. For each call after the first, you will need to pass in the results of the previous call.\n",
      "\n",
      "This is a step up from yesterday\u2019s mode of working where a single Map/Reduce call would solve your problem.\n",
      "The structure of this extra assignment is as follows:\n",
      "\n",
      "1. You will first look at the code we already provide for you.\n",
      "2. Based on it, we will run a version that is not distributed.\n",
      "3. Wewillconvertthistoadistributedversionthatisna \u0308\u0131veandveryinefficient. 4. We will then improve this version to be more efficient.\n",
      "5. Finally, we will run this code on a large dataset.\n",
      "\n",
      "This is a rather large set of exercises. Don\u2019t feel discouraged if you cannot complete everything.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "First MapReduce Version"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the directory ```distributed_em``` you will find a few data and code files:\n",
      "\n",
      "- ```sequences200.txt``` contains the first 200 sequences.\n",
      "\n",
      "\n",
      "- ```word_tag_dict.pkl``` is an auxiliary file containing the dictionaries of words and tags. \n",
      "\n",
      "\n",
      "- ```emstep.py``` contains a skeleton of the code we will be writing.\n",
      "\n",
      "\n",
      "- A few other files we will use below (ignore them for the moment)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      " Loading data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The datafiles are in a different format from what was used in the previous EM tutorial because the simplest way to use Hadoop (which is the underlying framework of Amazon EC2) is make each mapper receive a single line of the file.\n",
      "\n",
      "To load a single line of the input file, we have provided in file ```emstep.py``` a function\n",
      "\n",
      "    def load_sequence(line, word_dict, tag_dict): \n",
      "        \"\"\"\n",
      "        seq = load_sequence(s, word_dict, tag_dict)\n",
      "        Load a sequence from a single line\n",
      "        word_dict & tag_dict should be loaded from the file ``word_tag_dict.pkl``\n",
      "        Parameters\n",
      "        ----------\n",
      "        s : str\n",
      "        word_dict : dict\n",
      "        tag_dict : dict\n",
      "        Returns\n",
      "        -------\n",
      "        seq : Sequence object\n",
      "        \"\"\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "which takes the input data and the metadata in the auxiliary file. Here is how you would use it to just count the number of tokens in the dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "from emstep import load_sequence\n",
      "\n",
      "path_data = path_inside_lxmls_toolkit_student + '/data/'\n",
      "word_dict, tag_dict = pickle.load(open(path_data + 'word_tag_dict.pkl'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n=0\n",
      "for line in open('sequences200.txt'):\n",
      "    s = load_sequence(line, word_dict, tag_dict)\n",
      "n += len(s)\n",
      "print 'Nr of tokens: ', n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Nr of tokens:  6\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Processing a single sequence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have also provided code for you to compute the statistics for a single sequence in the function\n",
      "\n",
      "    def predict_sequence(sequence, hmm):\n",
      "        \"\"\"\n",
      "        log_likelihood, initial_counts, transition_counts, final_counts,\\\n",
      "                emission_counts = predict_sequence(seq, hmm)\n",
      "        Run forward-backward on a single sentence.\n",
      "        Parameters\n",
      "        ----------\n",
      "        seq : Sequence object\n",
      "        hmm: HMM object\n",
      "        Returns\n",
      "        -------\n",
      "        log_likelihood : float\n",
      "        initial_counts : np.ndarray\n",
      "        transition_counts : ndarray\n",
      "        final_counts : ndarray\n",
      "        emission_counts : ndarray\n",
      "        \"\"\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is how you could use it. First we need to initialize an HMM object:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import lxmls.sequences.hmm as hmmc\n",
      "import pickle\n",
      "word_dict, tag_dict = pickle.load(open('word_tag_dict.pkl')) \n",
      "hmm = hmmc.HMM(word_dict, tag_dict)\n",
      "hmm.initialize_random()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we also made use of the ```word_tag_dict.pkl``` file to get the dictionaries.\n",
      "\n",
      "Then, we initialized the HMM with a random initialization.\n",
      "Now, we can use the ```predict_sequence``` function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from emstep import load_sequence,predict_sequence\n",
      "for line in open('sequences200.txt'):\n",
      "    seq = load_sequence(line, word_dict, tag_dict) \n",
      "    statistics = predict_sequence(seq, hmm)\n",
      "\n",
      "print statistics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(-60.093189243321, array([ 0.06070256,  0.10681945,  0.07472935,  0.06433488,  0.05881742,\n",
        "        0.07103551,  0.08233218,  0.11874817,  0.10168097,  0.10845972,\n",
        "        0.05237482,  0.09996496]), array([[ 0.03565297,  0.04666479,  0.03671059,  0.02873311,  0.03120076,\n",
        "         0.03342427,  0.04786656,  0.0347694 ,  0.04752365,  0.02841624,\n",
        "         0.0345838 ,  0.03023757],\n",
        "       [ 0.03100078,  0.04085164,  0.03305837,  0.03950368,  0.04502725,\n",
        "         0.03956245,  0.04728029,  0.0386909 ,  0.0468984 ,  0.03907941,\n",
        "         0.0307449 ,  0.04389172],\n",
        "       [ 0.03846765,  0.03450405,  0.03450473,  0.02732199,  0.04021879,\n",
        "         0.03664889,  0.03865947,  0.03963521,  0.02964498,  0.0326164 ,\n",
        "         0.02319518,  0.03126524],\n",
        "       [ 0.04411486,  0.03278171,  0.02994876,  0.03045874,  0.02859975,\n",
        "         0.04715191,  0.04904154,  0.03310819,  0.03421841,  0.03181844,\n",
        "         0.02515771,  0.0248943 ],\n",
        "       [ 0.04882806,  0.03797992,  0.0457929 ,  0.04386734,  0.03979745,\n",
        "         0.04741653,  0.02998195,  0.04893787,  0.03648465,  0.03745717,\n",
        "         0.03127318,  0.05218046],\n",
        "       [ 0.04245356,  0.03395439,  0.03206517,  0.04352603,  0.04154808,\n",
        "         0.03023838,  0.03742158,  0.04369984,  0.05202514,  0.04527774,\n",
        "         0.02510892,  0.03349704],\n",
        "       [ 0.04145803,  0.05243259,  0.02393475,  0.03367575,  0.03642822,\n",
        "         0.03238374,  0.04721922,  0.042297  ,  0.04231178,  0.03378545,\n",
        "         0.02859079,  0.02793581],\n",
        "       [ 0.02678916,  0.04585951,  0.02977378,  0.02813441,  0.04266058,\n",
        "         0.03301622,  0.02905808,  0.02555877,  0.03196236,  0.04144921,\n",
        "         0.0200572 ,  0.02483687],\n",
        "       [ 0.02869131,  0.04168153,  0.04840491,  0.02619307,  0.03029384,\n",
        "         0.05184872,  0.04359525,  0.03071538,  0.03910572,  0.04183841,\n",
        "         0.03463432,  0.03507681],\n",
        "       [ 0.03337331,  0.0444074 ,  0.02401297,  0.02786876,  0.02769776,\n",
        "         0.02852809,  0.02670118,  0.03079588,  0.04767921,  0.02102859,\n",
        "         0.0232048 ,  0.02395161],\n",
        "       [ 0.02393825,  0.03893069,  0.02878471,  0.01750917,  0.02311755,\n",
        "         0.02167575,  0.03034399,  0.02570138,  0.03424742,  0.02879873,\n",
        "         0.01429885,  0.03364236],\n",
        "       [ 0.02691601,  0.02615026,  0.0234471 ,  0.036827  ,  0.0267223 ,\n",
        "         0.03800719,  0.02342503,  0.02463303,  0.03628975,  0.02779241,\n",
        "         0.02775989,  0.0379393 ]]), array([ 0.07480232,  0.10621077,  0.09097319,  0.09201016,  0.14550259,\n",
        "        0.09194923,  0.07419115,  0.07936148,  0.07536878,  0.05835111,\n",
        "        0.05475412,  0.05652511]), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       ..., \n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]))\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is important to note that ```predict_sequence``` is a pure function! It does not change its inputs. This means that if you call it in a different order or in different machines, you will always get the same results.\n",
      "\n",
      "\n",
      "This also explains why we need to have the word dictionaries precomputed: if we built them as we go, then the order in which the sequences are processed would make a difference. Thus, we could not process the sequences in parallel.\n",
      "\n",
      "We will comment on this later when we see an alternative (more sophisticated implementation). In our case, we were able to compute the dictionaries using a simple Python script, but if the data was truly large, we could have written another MapReduce job to discover all the words in the dataset.\n",
      "\n",
      "If you look back to equations (2.51)\u2013(2.54) you can see that ```predict_sequence``` is computing the value inside the outer sums."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Combining Partial Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, you should understand the code we provided.\n",
      "The goal of the next exercise is to write a function called ```combine_partials``` that can take all the sequence\n",
      "statistics and output the final statistics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import lxmls.sequences.hmm as hmmc\n",
      "import pickle\n",
      "from emstep import load_sequence,predict_sequence,combine_partials\n",
      "word_dict, tag_dict = pickle.load(open('word_tag_dict.pkl')) \n",
      "hmm = hmmc.HMM(word_dict, tag_dict)\n",
      "hmm.initialize_random()\n",
      "statistics = []\n",
      "\n",
      "\n",
      "for line in open('sequences200.txt'):\n",
      "    seq = load_sequence(line, word_dict, tag_dict) \n",
      "    statistics.append(predict_sequence(seq, hmm))\n",
      "\n",
      "final = combine_partials(statistics, hmm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 5.1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Write the function combine_partials. \n",
      "\n",
      "This function should take a list of all the statistics and an HMM object. \n",
      "\n",
      "It should modify the HMM object to reflect the results of all the sequences.\n",
      "\n",
      "Your function should perform some computations and then assign to the hmm object:\n",
      "    \n",
      "    def combine_partials(statistics, hmm):\n",
      "        hmm.log_likelihood = #write here...\n",
      "        hmm.initial_counts = #write here...\n",
      "\ufffc\ufffc\ufffc\n",
      "A template is provided in emstep.py.\n",
      "\n",
      "Note that this separation into sequence statistics and combination does not really correspond to the expectation and maximisation steps.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The students need to write this:\n",
      "def combine_partials(counts, hmm):\n",
      "    \"\"\"\n",
      "    combine_partials(counts, hmm)\n",
      "\n",
      "    This function should combine the results of calling predict_sequence many\n",
      "    times and assign to the hmm member objects\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    counts : list of tuples\n",
      "        This is a list of results from the ``predict_sequence`` functions\n",
      "\n",
      "    \"\"\"\n",
      "    hmm.log_likelihood = 0\n",
      "    hmm.initial_counts = 0\n",
      "    hmm.transition_counts = 0\n",
      "    hmm.emission_counts = 0\n",
      "    hmm.final_counts = 0\n",
      "    for partial in counts:\n",
      "        hmm.log_likelihood += partial[0]\n",
      "        hmm.initial_counts += partial[1]\n",
      "        hmm.transition_counts += partial[2]\n",
      "        hmm.final_counts += partial[3]\n",
      "        hmm.emission_counts += partial[4]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The previous exercise resulted in code that was not distributed, but already had the map/reduce structure we needed to make it work.\n",
      "For now, the reduce step is not distributed (this will be improved in the next exercises).\n",
      "\n",
      "We will perform a complete mapreduce run for each iteration of EM that we compute. In order to run multiple iterations, we will run mapreduce repeatedly.\n",
      "In what follows, we will save our output to a file called ```hmm.pkl``` on each iteration and then load it from there on the next iteration. Naturally, the first iteration needs to be a special case and we initialize randomly that time.\n",
      "\n",
      "\n",
      "We will also output a Python object from our reduce method. For this, we need a bit of magic (which is filled in the template we provide and now explain):\n",
      "\n",
      "    class EMStep(MRJob):\n",
      "    INTERNAL_PROTOCOL = PickleProtocol OUTPUT_PROTOCOL = PickleValueProtocol\n",
      "    def reducer(self, _, partials): ...\n",
      "            yield 'result', a_python_object"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By declaring our output protocol to be ```PickleValueProtocol```, this means that we can emit a Python object in the reduce as the final output and it will be properly serialized to the output.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 5.2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on your function combine_partials and the code we provided you, fill in the map and reduce steps.\n",
      "Forthemoment,your ```reduce``` function should have a single emission of the form yield \u2019```result```\u2019, hmm. Later, we will see more sophisticated methods.\n",
      "You may want to read the next few paragraphs before you start.\n",
      "\n",
      "In order to test this code on the cluster, you need to run it with the following flags (we are also directing the output to the file ```next_iteration.pkl```):\n",
      "\n",
      "\n",
      "    python emstep.py \\\n",
      "        --file=word_tag_dict.pkl\\\n",
      "        sequences200.txt > next_iteration.pkl\n",
      "   \n",
      "The first argument (```--file=word_tag_dict.pkl```) declares that the file ```word_tag_dict.pkl``` is needed to run the job.\n",
      "In order to run multiple iterations, you need to save the results of the previous iteration to a file, which can be loaded at startup.\n",
      "So, once you have run the code the first time, rename the output file by typing this into the terminal:\n",
      "\n",
      "    mv next_iteration.pkl hmm.pkl\n",
      "   \n",
      "Notice that in the **\\_\\_init__** function we check whether the file hmm.pkl exists and load it if so:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import path\n",
      "if path.exists('hmm.pkl'):\n",
      "hmm = pickle.load(open('hmm.pkl').read().decode('string-escape')) else:\n",
      "    hmm = hmmc.HMM(word_dict, tag_dict)\n",
      "    hmm.initialize_random()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "expected an indented block (<ipython-input-79-837f5f76229b>, line 3)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-79-837f5f76229b>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    hmm = pickle.load(open('hmm.pkl').read().decode('string-escape')) else:\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We needed to be careful with the string escapes in the above code, but otherwise were able to just rely on Python pickle to load the results.\n",
      "And you can now call it again (passing the hmm.pkl file on the command line!):\n",
      "\n",
      "    python emstep.py \\\n",
      "        --file=word_tag_dict.pkl \\\n",
      "        --file=hmm.pkl \\\n",
      "        sequences200.txt > next_iteration.pkl\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pitfall: Be careful to not over-write your HMM file! You need to perform two steps:\n",
      "\n",
      "1. Run the code, outputing to a temporary file.\n",
      "2. Rename the temporary file.\n",
      "\n",
      "The following is a mistake!\n",
      "\n",
      "    python emstep.py \\\n",
      "        --file=word_tag_dict.pkl \\\n",
      "        --file=hmm.pkl \\\n",
      "        sequences200.txt > hmm.pkl\n",
      "        \n",
      "This is a mistake because the hmm.pkl is now removed (to make space for the new output) before your program has a chance to read it!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Introducing mapper final"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While the code you wrote in the last exercise works correctly, it is horribly inefficient. The problem is that we output several matrices for each sequence. This results in too much communication between processes and the reduce step needs to load all these large matrices.\n",
      "In order to address this problem we need to introduce a new operation, mapper_final. This is called, for each mapper, when all the input has been processed.\n",
      "You can perhaps understand this by imagining that mrjob is running the following loop (in Python pseudo- code):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for key,value in input: \n",
      "    job.mapper(key,value)\n",
      "job.mapper_final()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'function' object is not iterable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-80-76570847da60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can delay emission of partial results until the ```mapper_final``` step and then emit the partial output of every sequence this job processed.\n",
      "For example, here is how WordCount can be performed using ```mapper_final```:\n",
      "\n",
      "    class WordCount(MRJob): \n",
      "\n",
      "        def __init__:\n",
      "            self.counts = {}\n",
      "\n",
      "        def mapper(self, _, values): \n",
      "            for word in values.split(): \n",
      "                if word in self.counts:\n",
      "                    self.counts[word] += 1 \n",
      "                else:\n",
      "                    \ufffcself.counts[word] = 1\n",
      "    \ufffc\n",
      "        \ufffcdef mapper_final(self): \n",
      "            for k in self.counts:\n",
      "                yield k, self.counts[k]\n",
      "\n",
      "        def reducer(self, key, values): \n",
      "            yield key, sum(values)\n",
      "  \n",
      "Note that the reduce method is still necessary: although each map job will emit the final result for all the documents it saw, we still need to combine the results from different map jobs (which run on different machines and processed different documents).\n",
      "\n",
      "\n",
      "By only emitting results in the ```mapper_final``` method, we have converted the implementation to use a batch system: the dataset is partioned into a block for each processor, each processor works on that whole block, and the reduction is only made to combine the results of different processors.\n",
      "This cuts down the communication overhead drastically and also makes the reduce function be faster and less resource intensive. Now, it only needs to work with a single set of statistics per compute node instead of one for each input sequence. Resource usage for reduction now only depends on the number of machines used and not the size of the input.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 5.3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use mapper_final to improve your MapReduce implementation. Initialize the matrices and log likelihood to zero in the __init__ constructor and update partial sums in the map function. Emit only in the ```mapper_final``` method.\n",
      "\n",
      "The ```reduce``` function should not need to be changed at all.\n",
      "\n",
      "Use the previous na\u00efve version as a benchmark. The results should not change beyond a rounding error (they may change slightly).\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parallelizing Reduce*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you ran out of time to complete this next section, don\u2019t worry, this section is an advanced module and you have already seen the major take-home points of the tutorial with the previous exercise.\n",
      "\n",
      "Our code can still be improved in two ways: (1) there is a single reduce call, which does not take advantage of the fact that we have a cluster of machines; and (2) since the emission matrices are sparse, we are emitting large matrices with many zeros.\n",
      "\n",
      "The previous code also had reduce use resources that grow with the number of processes. This may still be too much if you use a thousand of machines. We will also avoid this problem.\n",
      "In order to parallelize reduce, we need to start emitting partial resultsat a more fine grained level. Here are the types of emissions we want to consider:\n",
      "\n",
      "1. The **initial counts**: this is per state.\n",
      "2. The **final counts**: this is per state again.\n",
      "3. The **transition counts**: this is for each pair of states (transition from A to B). 4. The emission counts: this is for each pair of state and word.\n",
      "5. The **log likelihood**.\n",
      "\n",
      "In order to distinguish all of these, we will emit them as numbers with different keys. For example, the log likelihood will simply have the key \u201clog likelihood\u201d, while the matrices will have keys which identify the matrix and the cell.\n",
      "The code below exemplifies how we could emit the log likelihood and the vector of final counts:\n",
      "\n",
      "    yield 'log likelihood', log_likelihood\n",
      "    \n",
      "    for i in xrange(len(counts)):\n",
      "        name = hmm.get_state_name(i) \n",
      "        yield 'final '+name, counts[i]\n",
      "        \n",
      "Note how we included the name of the state in the key. For the transition counts and emission counts, we will need a nested ```for`` loop:\n",
      "\n",
      "    for i in xrange(transition.shape[0]):\n",
      "        for j in xrange(transition.shape[1]):\n",
      "            if transition[i,j]: # <----- IGNORE ZEROS\n",
      "                name_i = hmm.get_state_name(i)\n",
      "                name_j = hmm.get_state_name(j)\n",
      "                yield 'transition %s %s' % (name_i, name_j), transition[i,j]\n",
      "                \n",
      "Again, we are including the names of the states in the key. The check for zeros is important as it is useless to output zero counts. If the universe of words is large, then those matrices will be sparse and we avoid useless computation and communication."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 5.4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look in the file ```emission_snippets.py```.This contains the for loops above and more.\n",
      "\n",
      "\n",
      "Use these to improve your ```mapper_final function```. Write the corresponding reduce function. Do not change the keys used for the emission as they are needed below (see the next paragraph after this exercise).\n",
      "\n",
      "\n",
      "**Important**: Now you should remove the ```OUTPUT_PROTOCOL``` declaration! See the note on section 5.5.4 on what\n",
      "this means.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The resulting output is no longer a single HMM object which we can load from Python, but a text file which encodes all the matrices. If you have used our snippets exactly, you can also use the code in the function ```parse_hmm_from_output``` to parse this file and generate a new HMM object.\n",
      "Note that with this output method, we do not need to have the word dictionaries precomputed. Whereas previously, we relied on matrix indeces to keep our words apart, now we output the actual word and therefore, we could just process each sequence as it comes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A Note on Hadoop Overhead and Big Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you probably noticed, Hadoop has a lot of overhead and each iteration takes a long time to start computing and finish running. In our case, this is a very significant part of the time it takes to run an iteration.\n",
      "\n",
      "Hadoop is heavy machinery: it takes a while to move, but then can be very powerful. The advantages are in the scaling: if we had a trillion sequences which would take thousands of computing hours to process, then the minute or so that it takes to start up would not matter and we would reap the benefits of working with hundreds, even thousands, of machines. We can say that Hadoop has high latency but can have high throughput as well. Thus, it is not very appropriate for small problems, but can scale to huge ones.\n",
      "\n",
      "Unfortunately, we only had a few hours in which you could work: including understanding the task, writing the code, debugging it and running it. Therefore, it was unfeasible to ask you to work on a problem with a million sequences which could only be tackled with the heavy machinery of Hadoop. We could not really work on very large problems. This was only a demo of what Big Data really is.\n",
      "However, the code you wrote at the end of the chapter is now perfectly scalable to any size project you want to tackle. The skills you learned can be applied to any web-scale problem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "path_data = path_inside_lxmls_toolkit_student + '/data/'\n",
      "open(path_data + 'word_tag_dict.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "<open file '/Users/davidbuchacaprats/Dropbox/lxmls2015/lxmls-toolkit-student/data/word_tag_dict.pkl', mode 'r' at 0x10ee7b150>"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The format of s is the same as Day 2. If you need to refresh your memory, try placing a line with import pdb; pdb.set trace() after the call to the function to see the format of s."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}