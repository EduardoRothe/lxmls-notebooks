{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi decoding\n",
    "\n",
    "**Viterbi decoding** consists in\n",
    "picking the best global hidden state sequence  $\\widehat{y}$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{y} = \\text{argmax}_{y \\in \\Lambda^N} P(Y=y|X=x) = \\text{argmax}_{y \\in \\Lambda^N} P(X=x,Y=y).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The Viterbi algorithm  is very similar to the forward procedure of the FB algorithm,\n",
    "making use of the same trellis structure to efficiently represent the exponential number of sequences without prohibitive computation costs. In fact, the only\n",
    "difference from the forward-backward algorithm is in the recursion where **instead of summing over all possible hidden states, we take their maximum**.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Viterbi }\\;\\;\\;\\;  \\mathrm{viterbi}(i, \\pmb{x}, y_i) = \\max_{y_1...y_{i-1}} P(Y_1=y_1,\\ldots Y_i = y_i , X_1=x_1,\\ldots, X_i=x_i)\n",
    "\\end{equation}\n",
    "\n",
    "The Viterbi trellis represents the path with maximum probability in\n",
    "position\n",
    "$i$ when we are in state $Y_i=y_i$ and that we have observed $x_1,\\ldots,x_i$\n",
    "up to that position. The Viterbi algorithm is defined by the\n",
    "following recurrence of the viterbi values: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{viterbi}(1, \\pmb{x}, c_k) &=& P_{\\mathrm{init}}(c_k|\\text{ start}) \\times \n",
    "P_{\\mathrm{emiss}}(x_1 | c_k)\n",
    " \\\\\n",
    " \\mathrm{viterbi}(i, \\pmb{x}, c_k) &=& \\left( \\displaystyle \\max_{c_l \\in \\Lambda} P_{\\mathrm{trans}}(c_k | c_l) \\times \\mathrm{viterbi}(i-1, \\pmb{x}, c_l) \\right) \\times P_{\\mathrm{emiss}}(x_i | c_k)\n",
    "  \\\\\n",
    "  \\mathrm{viterbi}(N+1, \\pmb{x}, \\text{ stop}) &=& \\max_{c_l \\in \\Lambda} P_{\\mathrm{final}}(\\text{ stop} | c_l) \\times \\mathrm{viterbi}(N, \\pmb{x}, c_l)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Once the viterbi value at the last position ``viterbi(N,x,c_l)`` is computed the algorithm can backtrack using the following recurrence\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{backtrack}(N+1, \\pmb{x}, \\text{ stop}) &=& \\text{argmax}_{c_l \\in \\Lambda} P_{\\mathrm{final}}(\\text{ stop} | c_l) \\times \\mathrm{viterbi}(N,\\pmb{x}, c_l).\n",
    " \\\\\n",
    "\\mathrm{backtrack}(i,\\pmb{x}, c_k) &=& \\left( \\displaystyle \\text{argmax}_{c_l \\in \\Lambda} P_{\\mathrm{trans}}(c_k | c_l) \\times \\mathrm{viterbi}(i-1,\\pmb{x}, c_l) \\right) \n",
    " \\end{eqnarray}\n",
    "\n",
    "The following  pseudo code  is the Viterbi algorithm.\n",
    "Note the similarity with the forward algorithm.\n",
    "The only differences are:\n",
    "\n",
    "- Maximizing instead of summing;\n",
    "- Keeping the argmax's to backtrack.\n",
    "\n",
    "<img src=\"../images_for_notebooks/day_2/viterbi.png\"  style=\"max-width:100%; width: 80%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # THIS CANT GO INTO THE STUDENT VERSION\n",
    "    def run_viterbi(self, initial_scores, transition_scores, final_scores, emission_scores):\n",
    "\n",
    "        length = np.size(emission_scores, 0) # Length of the sequence.\n",
    "        num_states = np.size(initial_scores) # Number of states.\n",
    "\n",
    "        # Viterbi variables.\n",
    "        viterbi = np.zeros([length, num_states]) + logzero()\n",
    "        backtrack = np.zeros([length, num_states], dtype=int)\n",
    "\n",
    "        # Initialization.\n",
    "        viterbi[0,:] = emission_scores[0,:] + initial_scores\n",
    "\n",
    "        # viterbi loop.\n",
    "        for pos in xrange(1,length):\n",
    "            for current_state in xrange(num_states):\n",
    "                # Note the fact that multiplication in log domain turns a sum and sum turns a logsum\n",
    "                viterbi_score = viterbi[pos-1, :] + transition_scores[pos-1, current_state, :]\n",
    "                viterbi[pos, current_state] = np.max(viterbi_score)\n",
    "                viterbi[pos, current_state] += emission_scores[pos, current_state]\n",
    "                backtrack[pos, current_state] = np.argmax(viterbi_score)\n",
    "\n",
    "        best_score = np.max(viterbi[-1, :] + final_scores)\n",
    "        \n",
    "        best_path = np.zeros(length, dtype=int)\n",
    "        best_path[-1] = np.argmax(viterbi[-1, :] + final_scores)\n",
    "\n",
    "        for pos in xrange(length-2,-1,-1):\n",
    "            #best_path[pos] = int(np.argmax(backtrack[pos+1]))\n",
    "            best_path[pos] = backtrack[pos+1, best_path[pos+1]]\n",
    "\n",
    "        return best_path , best_score \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the method  ``run_viterbi`` for performing Viterbi decoding\n",
    "in file ```lxmls/sequences/sequence_classification_decoder.py.```**\n",
    "\n",
    "This method at the moment raises a NotImplementedError\n",
    "\n",
    "    def run_viterbi(self, initial_scores,transition_scores,final_scores,emission_scores):\n",
    "        # Complete Exercise 2.8 \n",
    "        raise NotImplementedError, \"Complete Exercise 2.8\" \n",
    "        # THIS FUNCTION SHOULD RETURN \n",
    "        # best_states, total_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "# We will this append to ensure we can import lxmls toolking\n",
    "sys.path.append('../../lxmls-toolkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use ```hmm.viterbi_decode``` to predict the sequence of tags for a given sequence of visible states. \n",
    "\n",
    "Notice that ```hmm.viterbi_decode``` ( which can be located in ```sequences/sequence_classifier```) uses the method ```SequenceClassificationDecoder.run_viterbi``` (from the folder ```sequences/sequence_classification_decoder```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "walk/rainy walk/rainy shop/sunny clean/sunny "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding Prediction test 0 with smoothing:\n",
      "\twalk/rainy walk/rainy shop/sunny clean/sunny  \n",
      "score:\n",
      "\t-6.02050124698\n",
      "\n",
      "A correct implementation of Viterbi decoding Prediction test 0 with smoothing\n",
      "should return:\n",
      "\twalk/rainy walk/rainy shop/sunny clean/sunny \n",
      "score:\n",
      "\t-6.02050124698\n"
     ]
    }
   ],
   "source": [
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[0])\n",
    "print \"Viterbi decoding Prediction test 0 with smoothing:\\n\\t\", y_pred, \"\\nscore:\\n\\t\",score\n",
    "\n",
    "print \"\\nA correct implementation of Viterbi decoding Prediction test 0 with smoothing\"\n",
    "print \"should return:\"\n",
    "print \"\\t\", \"walk/rainy walk/rainy shop/sunny clean/sunny \"\n",
    "print \"score:\"\n",
    "print \"\\t\",-6.02050124698"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth test 0:\n",
      "\twalk/rainy walk/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "print \"Truth test 0:\\n\\t\", simple.test.seq_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding Prediction test 1 with smoothing:\n",
      "\tclean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "score:\n",
      "\t-11.713974074\n",
      "\n",
      "A correct implementation of Viterbi decoding Prediction test 0 with smoothing\n",
      "should return\n",
      "\tclean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "score:\n",
      "\t-11.713974074\n"
     ]
    }
   ],
   "source": [
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[1])\n",
    "print \"Viterbi decoding Prediction test 1 with smoothing:\\n\\t\", y_pred\n",
    "print \"score:\"\n",
    "print \"\\t\",score\n",
    "\n",
    "print \"\\nA correct implementation of Viterbi decoding Prediction test 0 with smoothing\"\n",
    "print \"should return\"\n",
    "print \"\\t\",simple.test.seq_list[1] \n",
    "print \"score:\"\n",
    "print \"\\t\",-11.713974074"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-Speech (PoS) tagging is one of the most important NLP tasks. The\n",
    "task is to assign each word a grammatical category, or Part-of-Speech, such as noun,\n",
    "verb, adjective,... Recalling the defined notation, $\\Sigma$ is a \n",
    "vocabulary of word types, and \n",
    "$\\Lambda$ is the set of Part-of-Speech tags.\n",
    "\n",
    "In English, using the Penn Treebank (PTB) corpus , the current\n",
    "state of the art for part of speech tagging is around 97\\% for a\n",
    "variety of methods.\n",
    "\n",
    "In the rest of this class we will use a subset of the PTB corpus, but\n",
    "instead of using the original 45 tags we will use a reduced tag set of\n",
    "12 tags, to make the algorithms faster for the\n",
    "class. In this task, $x$ is a sentence (for example, a sequence of word tokens) and $y$\n",
    "is the sequence of possible PoS tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.pos_corpus as pcc\n",
    "corpus = pcc.PostagCorpus()\n",
    "\n",
    "#path_to_data = path_inside_lxmls_toolkit_student + \"/data/\"\n",
    "path_to_data = \"../../lxmls-toolkit/data/\"\n",
    "\n",
    "train_seq = corpus.read_sequence_list_conll(path_to_data + \"train-02-21.conll\",max_sent_len=15,max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(path_to_data + \"test-23.conll\",max_sent_len=15,max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(path_to_data + \"dev-22.conll\",max_sent_len=15,max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The/det luxury/noun auto/noun maker/noun last/adj year/noun sold/verb 1,214/num cars/noun in/adp the/det U.S./noun "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hmmc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1e78320769b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_supervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_transition_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hmmc' is not defined"
     ]
    }
   ],
   "source": [
    "hmm = hmmc.HMM(corpus.word_dict, corpus.tag_dict)\n",
    "hmm.train_supervised(train_seq)\n",
    "hmm.print_transition_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the transition probabilities of the trained model,\n",
    "and see if they match your intuition about the English language \n",
    "(e.g. adjectives tend to come before nouns). Each column is the previous state and row is the current state. Note the high probability of having Noun after Determinant or Adjective, or of having Verb after Nouns or Pronouns, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model using both posterior decoding and Viterbi decoding on\n",
    "both the train and test set, using the methods in class HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: Posterior Decode 0.985, Viterbi Decode: 0.985\n"
     ]
    }
   ],
   "source": [
    "viterbi_pred_train = hmm.viterbi_decode_corpus(train_seq) \n",
    "posterior_pred_train = hmm.posterior_decode_corpus(train_seq)\n",
    "eval_viterbi_train = hmm.evaluate_corpus(train_seq, viterbi_pred_train)\n",
    "eval_posterior_train = hmm.evaluate_corpus(train_seq, posterior_pred_train)\n",
    "print \"Train Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(eval_posterior_train,eval_viterbi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: Posterior Decode 0.350, Viterbi Decode: 0.509\n"
     ]
    }
   ],
   "source": [
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq) \n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq) \n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq,viterbi_pred_test)\n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq,posterior_pred_test) \n",
    "print \"Test Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(\n",
    "    eval_posterior_test,eval_viterbi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- What do you observe? \n",
    "\n",
    "Remake the previous exercise but now train the HMM using smoothing.\n",
    "Try different values (0,0.1,0.01,1) and report the results on the train and \n",
    "development set. (Use function pick best smoothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing 10.000000 --  Train Set Accuracy: Posterior Decode 0.731, Viterbi Decode: 0.691\n",
      "Smoothing 10.000000 -- Test Set Accuracy: Posterior Decode 0.712, Viterbi Decode: 0.675\n",
      "Smoothing 1.000000 --  Train Set Accuracy: Posterior Decode 0.887, Viterbi Decode: 0.865\n",
      "Smoothing 1.000000 -- Test Set Accuracy: Posterior Decode 0.818, Viterbi Decode: 0.792\n",
      "Smoothing 0.100000 --  Train Set Accuracy: Posterior Decode 0.968, Viterbi Decode: 0.965\n",
      "Smoothing 0.100000 -- Test Set Accuracy: Posterior Decode 0.851, Viterbi Decode: 0.842\n",
      "Smoothing 0.000000 --  Train Set Accuracy: Posterior Decode 0.985, Viterbi Decode: 0.985\n",
      "Smoothing 0.000000 -- Test Set Accuracy: Posterior Decode 0.370, Viterbi Decode: 0.526\n"
     ]
    }
   ],
   "source": [
    "best_smoothing = hmm.pick_best_smoothing(train_seq, dev_seq, [10,1,0.1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Smoothing 0.100000 -- Test Set Accuracy: Posterior Decode 0.837, Viterbi Decode: 0.827\n"
     ]
    }
   ],
   "source": [
    "hmm.train_supervised(train_seq, smoothing=best_smoothing)\n",
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq)\n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq, viterbi_pred_test) \n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq, posterior_pred_test)\n",
    "print \"Best Smoothing %f -- Test Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(best_smoothing,eval_posterior_test,eval_viterbi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Perform some error analysis to understand were the errors are coming from. \n",
    "\n",
    " You can start by visualizing the confusion matrix (true tags vs predicted tags). \n",
    "\n",
    "You should get something like what is shown in Figure 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fd35410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lxmls.sequences.confusion_matrix as cm\n",
    "import matplotlib.pyplot as plt\n",
    "confusion_matrix = cm.build_confusion_matrix(test_seq.seq_list, viterbi_pred_test, len(corpus.tag_dict), hmm.get_num_states()) \n",
    "cm.plot_confusion_bar_graph(confusion_matrix, corpus.tag_dict, xrange(hmm.get_num_states()), 'Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  (OPTIONAL) Unsupervised Learning of HMMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you have made it so far you are awesome! \n",
    "\n",
    "> Don't worry the next couple of exercices do not require to actually code anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next address the problem of unsupervised learning. In this setting, \n",
    "we are not given any labeled data.\n",
    "\n",
    "All we get to see is a set of natural language sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the method to update the counts given the state and transition posteriors\n",
    "\n",
    "```\n",
    "def update_counts(self, sequence, state_posteriors, transition_posteriors):\n",
    "```\n",
    "\n",
    "Look at the code for EM algorithm in file ```sequences/hmm.py``` and check it for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def train_EM(self, dataset, smoothing=0, num_epochs=10, evaluate=True):\n",
    "        self.initialize_random()\n",
    "\n",
    "        if evaluate:\n",
    "            acc = self.evaluate_EM(dataset)\n",
    "            print \"Initial accuracy: %f\"%(acc)\n",
    "\n",
    "        for t in xrange(1, num_epochs):\n",
    "            #E-Step\n",
    "            total_log_likelihood = 0.0\n",
    "            self.clear_counts(smoothing)\n",
    "            for sequence in dataset.seq_list:\n",
    "                # Compute scores given the observation sequence.\n",
    "                initial_scores, transition_scores, final_scores, emission_scores = \\\n",
    "                    self.compute_scores(sequence)\n",
    "\n",
    "                state_posteriors, transition_posteriors, log_likelihood = \\\n",
    "                    self.compute_posteriors(initial_scores,\n",
    "                                            transition_scores,\n",
    "                                            final_scores,\n",
    "                                            emission_scores)\n",
    "                self.update_counts(sequence, state_posteriors, transition_posteriors)\n",
    "                total_log_likelihood += log_likelihood\n",
    "\n",
    "            print \"Iter: %i Log Likelihood: %f\"%(t, total_log_likelihood)\n",
    "            #M-Step\n",
    "            self.compute_parameters()\n",
    "            if evaluate:\n",
    "                 ### Evaluate accuracy at this iteration\n",
    "                acc = self.evaluate_EM(dataset)\n",
    "                print \"Iter: %i Accuracy: %f\"%(t,acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 20 epochs of the EM algorithm for part of speech induction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hmm.train_EM(train_seq, 0.1, 20, evaluate=True)\n",
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq) \n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq, viterbi_pred_test)\n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq, posterior_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix = cm.build_confusion_matrix(test_seq.seq_list, viterbi_pred_test, len(corpus.tag_dict), hmm.get_num_states())\n",
    "\n",
    "cm.plot_confusion_bar_graph(confusion_matrix, corpus.tag_dict, xrange(hmm.get_num_states()), 'Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
